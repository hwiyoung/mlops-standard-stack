#!/usr/bin/env python3
"""
MinIO ë°ì´í„° ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
ë¡œì»¬ í´ë”ì˜ íŒŒì¼ë“¤ì„ MinIO ë²„í‚·ìœ¼ë¡œ ì¬ê·€ì ìœ¼ë¡œ ì—…ë¡œë“œ

ì‚¬ìš©ë²•:
    python scripts/upload_data.py --source ./data/raw --bucket raw-data
    python scripts/upload_data.py -s ./data/images -b raw-data --skip-existing
"""

import argparse
import os
import sys
from pathlib import Path
from typing import List, Optional, Tuple

import boto3
from botocore.exceptions import ClientError
from dotenv import load_dotenv
from tqdm import tqdm


def load_minio_config() -> dict:
    """
    .env íŒŒì¼ì—ì„œ MinIO ì ‘ì† ì •ë³´ ë¡œë“œ
    
    Returns:
        MinIO ì„¤ì • ë”•ì…”ë„ˆë¦¬
    """
    # .env íŒŒì¼ ë¡œë“œ
    env_path = Path(__file__).parent.parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)
    
    config = {
        "endpoint_url": os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://localhost:9000"),
        "access_key": os.getenv("AWS_ACCESS_KEY_ID", os.getenv("MINIO_ROOT_USER", "minioadmin")),
        "secret_key": os.getenv("AWS_SECRET_ACCESS_KEY", os.getenv("MINIO_ROOT_PASSWORD", "minio_secure_password_2024")),
    }
    
    return config


def create_s3_client(config: dict):
    """
    boto3 S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„± (MinIO í˜¸í™˜)
    
    Args:
        config: MinIO ì„¤ì •
    
    Returns:
        boto3 S3 í´ë¼ì´ì–¸íŠ¸
    """
    return boto3.client(
        "s3",
        endpoint_url=config["endpoint_url"],
        aws_access_key_id=config["access_key"],
        aws_secret_access_key=config["secret_key"],
    )


def get_local_files(source_dir: Path, extensions: Optional[List[str]] = None) -> List[Path]:
    """
    ë¡œì»¬ í´ë”ì—ì„œ íŒŒì¼ ëª©ë¡ ì¬ê·€ì ìœ¼ë¡œ ìˆ˜ì§‘
    
    Args:
        source_dir: ì†ŒìŠ¤ ë””ë ‰í† ë¦¬
        extensions: í•„í„°ë§í•  í™•ì¥ì (Noneì´ë©´ ëª¨ë“  íŒŒì¼)
    
    Returns:
        íŒŒì¼ ê²½ë¡œ ë¦¬ìŠ¤íŠ¸
    """
    if not source_dir.exists():
        raise FileNotFoundError(f"ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {source_dir}")
    
    files = []
    for path in source_dir.rglob("*"):
        if path.is_file():
            if extensions is None:
                files.append(path)
            elif path.suffix.lower() in extensions:
                files.append(path)
    
    return sorted(files)


def check_object_exists(s3_client, bucket: str, key: str) -> bool:
    """
    MinIOì— ê°ì²´ê°€ ì´ë¯¸ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸
    
    Args:
        s3_client: S3 í´ë¼ì´ì–¸íŠ¸
        bucket: ë²„í‚·ëª…
        key: ê°ì²´ í‚¤
    
    Returns:
        ì¡´ì¬ ì—¬ë¶€
    """
    try:
        s3_client.head_object(Bucket=bucket, Key=key)
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "404":
            return False
        raise


def upload_file_with_progress(
    s3_client,
    local_path: Path,
    bucket: str,
    key: str,
    pbar: tqdm
) -> bool:
    """
    íŒŒì¼ì„ MinIOì— ì—…ë¡œë“œ (ì§„í–‰ë¥  ì½œë°± í¬í•¨)
    
    Args:
        s3_client: S3 í´ë¼ì´ì–¸íŠ¸
        local_path: ë¡œì»¬ íŒŒì¼ ê²½ë¡œ
        bucket: ëŒ€ìƒ ë²„í‚·
        key: ê°ì²´ í‚¤
        pbar: tqdm í”„ë¡œê·¸ë ˆìŠ¤ ë°”
    
    Returns:
        ì—…ë¡œë“œ ì„±ê³µ ì—¬ë¶€
    """
    file_size = local_path.stat().st_size
    
    def progress_callback(bytes_transferred):
        pbar.update(bytes_transferred)
    
    try:
        s3_client.upload_file(
            str(local_path),
            bucket,
            key,
            Callback=progress_callback
        )
        return True
    except Exception as e:
        print(f"\nâŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {local_path} - {e}")
        return False


def upload_directory(
    source_dir: str,
    bucket: str,
    prefix: str = "",
    skip_existing: bool = False,
    extensions: Optional[List[str]] = None,
    dry_run: bool = False
) -> Tuple[int, int, int]:
    """
    ë””ë ‰í† ë¦¬ë¥¼ MinIO ë²„í‚·ìœ¼ë¡œ ì—…ë¡œë“œ
    
    Args:
        source_dir: ì†ŒìŠ¤ ë””ë ‰í† ë¦¬
        bucket: ëŒ€ìƒ ë²„í‚·
        prefix: ë²„í‚· ë‚´ ê²½ë¡œ prefix
        skip_existing: ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ìŠ¤í‚µ
        extensions: í•„í„°ë§í•  í™•ì¥ì
        dry_run: ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ
    
    Returns:
        (ì—…ë¡œë“œëœ íŒŒì¼ ìˆ˜, ìŠ¤í‚µëœ íŒŒì¼ ìˆ˜, ì‹¤íŒ¨í•œ íŒŒì¼ ìˆ˜)
    """
    source_path = Path(source_dir).resolve()
    
    print("=" * 60)
    print("ğŸ“¤ MinIO ë°ì´í„° ì—…ë¡œë“œ")
    print("=" * 60)
    
    # MinIO ì„¤ì • ë¡œë“œ
    config = load_minio_config()
    print(f"ğŸ“¡ MinIO Endpoint: {config['endpoint_url']}")
    print(f"ğŸª£ Target Bucket: {bucket}")
    print(f"ğŸ“ Source Directory: {source_path}")
    if prefix:
        print(f"ğŸ“‚ Prefix: {prefix}")
    print()
    
    # S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±
    s3_client = create_s3_client(config)
    
    # ë²„í‚· ì¡´ì¬ í™•ì¸
    try:
        s3_client.head_bucket(Bucket=bucket)
    except ClientError as e:
        error_code = e.response["Error"]["Code"]
        if error_code == "404":
            print(f"âŒ ë²„í‚·ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {bucket}")
            print(f"   ë¨¼ì € ë²„í‚·ì„ ìƒì„±í•˜ì„¸ìš”: docker-compose up -d")
            return 0, 0, 0
        raise
    
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    print("ğŸ” íŒŒì¼ ìŠ¤ìº” ì¤‘...")
    files = get_local_files(source_path, extensions)
    
    if not files:
        print("âš ï¸  ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0, 0
    
    # ì´ í¬ê¸° ê³„ì‚°
    total_size = sum(f.stat().st_size for f in files)
    print(f"   ğŸ“Š ì´ {len(files)}ê°œ íŒŒì¼, {total_size / (1024*1024):.2f} MB")
    print()
    
    if dry_run:
        print("ğŸ§ª Dry Run ëª¨ë“œ - ì‹¤ì œ ì—…ë¡œë“œ ì—†ìŒ")
        for f in files[:10]:  # ì²˜ìŒ 10ê°œë§Œ í‘œì‹œ
            rel_path = f.relative_to(source_path)
            key = f"{prefix}/{rel_path}".lstrip("/") if prefix else str(rel_path)
            print(f"   ğŸ“„ {key}")
        if len(files) > 10:
            print(f"   ... ì™¸ {len(files) - 10}ê°œ íŒŒì¼")
        return len(files), 0, 0
    
    # ì—…ë¡œë“œ ì‹¤í–‰
    uploaded = 0
    skipped = 0
    failed = 0
    
    print("ğŸ“¤ ì—…ë¡œë“œ ì‹œì‘...")
    
    with tqdm(total=total_size, unit="B", unit_scale=True, desc="ì „ì²´ ì§„í–‰ë¥ ") as pbar:
        for local_file in files:
            rel_path = local_file.relative_to(source_path)
            key = f"{prefix}/{rel_path}".lstrip("/") if prefix else str(rel_path)
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ìŠ¤í‚µ
            if skip_existing and check_object_exists(s3_client, bucket, key):
                skipped += 1
                pbar.update(local_file.stat().st_size)
                continue
            
            # ì—…ë¡œë“œ
            if upload_file_with_progress(s3_client, local_file, bucket, key, pbar):
                uploaded += 1
            else:
                failed += 1
    
    # ê²°ê³¼ ì¶œë ¥
    print()
    print("=" * 60)
    print("âœ… ì—…ë¡œë“œ ì™„ë£Œ!")
    print("=" * 60)
    print(f"   ğŸ“¤ ì—…ë¡œë“œë¨: {uploaded}ê°œ")
    print(f"   â­ï¸  ìŠ¤í‚µë¨: {skipped}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {failed}ê°œ")
    print()
    print(f"ğŸ“Œ MinIO Consoleì—ì„œ í™•ì¸: http://localhost:9001/browser/{bucket}")
    
    return uploaded, skipped, failed


def main():
    parser = argparse.ArgumentParser(
        description="ë¡œì»¬ íŒŒì¼ì„ MinIO ë²„í‚·ìœ¼ë¡œ ì—…ë¡œë“œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì—…ë¡œë“œ
  python scripts/upload_data.py --source ./data/raw --bucket raw-data

  # prefix ì§€ì •
  python scripts/upload_data.py -s ./images -b raw-data --prefix project1/images

  # ì´ë¯¸ ìˆëŠ” íŒŒì¼ ìŠ¤í‚µ
  python scripts/upload_data.py -s ./data -b raw-data --skip-existing

  # íŠ¹ì • í™•ì¥ìë§Œ ì—…ë¡œë“œ
  python scripts/upload_data.py -s ./data -b raw-data --extensions .tif .tiff .jpg

  # Dry run (í…ŒìŠ¤íŠ¸)
  python scripts/upload_data.py -s ./data -b raw-data --dry-run
        """
    )
    
    parser.add_argument(
        "--source", "-s",
        type=str,
        required=True,
        help="ì—…ë¡œë“œí•  ë¡œì»¬ ë””ë ‰í† ë¦¬ ê²½ë¡œ"
    )
    
    parser.add_argument(
        "--bucket", "-b",
        type=str,
        default="raw-data",
        help="ëŒ€ìƒ MinIO ë²„í‚· (ê¸°ë³¸ê°’: raw-data)"
    )
    
    parser.add_argument(
        "--prefix", "-p",
        type=str,
        default="",
        help="ë²„í‚· ë‚´ ê²½ë¡œ prefix (ì˜ˆ: project1/images)"
    )
    
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        help="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ì€ ìŠ¤í‚µ"
    )
    
    parser.add_argument(
        "--extensions", "-e",
        type=str,
        nargs="*",
        default=None,
        help="ì—…ë¡œë“œí•  íŒŒì¼ í™•ì¥ì (ì˜ˆ: .tif .jpg .png)"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ í…ŒìŠ¤íŠ¸ë§Œ ìˆ˜í–‰"
    )
    
    args = parser.parse_args()
    
    # í™•ì¥ì ì²˜ë¦¬
    extensions = None
    if args.extensions:
        extensions = [ext if ext.startswith(".") else f".{ext}" for ext in args.extensions]
    
    # ì—…ë¡œë“œ ì‹¤í–‰
    upload_directory(
        source_dir=args.source,
        bucket=args.bucket,
        prefix=args.prefix,
        skip_existing=args.skip_existing,
        extensions=extensions,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()
