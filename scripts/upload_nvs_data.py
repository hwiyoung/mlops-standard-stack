#!/usr/bin/env python3
"""
NVS ë°ì´í„° ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸
COLMAP ì²˜ë¦¬ëœ í´ë” êµ¬ì¡°ë¥¼ ìœ ì§€í•˜ë©´ì„œ MinIOì— ì—…ë¡œë“œ

í´ë” êµ¬ì¡° ì˜ˆì‹œ:
    project_folder/
    â”œâ”€â”€ images/
    â”‚   â”œâ”€â”€ image_001.jpg
    â”‚   â”œâ”€â”€ image_002.jpg
    â”‚   â””â”€â”€ ...
    â”œâ”€â”€ sparse/
    â”‚   â””â”€â”€ 0/
    â”‚       â”œâ”€â”€ cameras.bin
    â”‚       â”œâ”€â”€ images.bin
    â”‚       â””â”€â”€ points3D.bin
    â””â”€â”€ (optional) dense/
        â””â”€â”€ ...

ì‚¬ìš©ë²•:
    python scripts/upload_nvs_data.py --source ./my_scene --project my_project
    python scripts/upload_nvs_data.py -s ./scene -p project_name --date 2024-01-15
"""

import argparse
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Optional, Tuple

import boto3
from botocore.exceptions import ClientError
from dotenv import load_dotenv
from tqdm import tqdm


def load_minio_config() -> dict:
    """
    .env íŒŒì¼ì—ì„œ MinIO ì ‘ì† ì •ë³´ ë¡œë“œ
    """
    env_path = Path(__file__).parent.parent / ".env"
    if env_path.exists():
        load_dotenv(env_path)
    
    config = {
        "endpoint_url": os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://localhost:9000"),
        "access_key": os.getenv("AWS_ACCESS_KEY_ID", os.getenv("MINIO_ROOT_USER", "minioadmin")),
        "secret_key": os.getenv("AWS_SECRET_ACCESS_KEY", os.getenv("MINIO_ROOT_PASSWORD", "minio_secure_password_2024")),
    }
    
    return config


def create_s3_client(config: dict):
    """boto3 S3 í´ë¼ì´ì–¸íŠ¸ ìƒì„±"""
    return boto3.client(
        "s3",
        endpoint_url=config["endpoint_url"],
        aws_access_key_id=config["access_key"],
        aws_secret_access_key=config["secret_key"],
    )


def ensure_bucket_exists(s3_client, bucket: str) -> bool:
    """ë²„í‚·ì´ ì—†ìœ¼ë©´ ìƒì„±"""
    try:
        s3_client.head_bucket(Bucket=bucket)
        return True
    except ClientError as e:
        error_code = e.response["Error"]["Code"]
        if error_code == "404":
            print(f"ğŸ“¦ ë²„í‚· ìƒì„± ì¤‘: {bucket}")
            s3_client.create_bucket(Bucket=bucket)
            return True
        raise


def validate_colmap_structure(source_dir: Path) -> dict:
    """
    COLMAP í´ë” êµ¬ì¡° ê²€ì¦
    
    Returns:
        ê²€ì¦ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
    """
    result = {
        "valid": True,
        "images_dir": None,
        "sparse_dir": None,
        "dense_dir": None,
        "warnings": [],
        "errors": []
    }
    
    # images í´ë” í™•ì¸
    images_dir = source_dir / "images"
    if images_dir.exists():
        image_files = list(images_dir.glob("*.[jJ][pP][gG]")) + \
                      list(images_dir.glob("*.[pP][nN][gG]")) + \
                      list(images_dir.glob("*.[jJ][pP][eE][gG]"))
        if image_files:
            result["images_dir"] = images_dir
            result["image_count"] = len(image_files)
        else:
            result["warnings"].append("images/ í´ë”ì— ì´ë¯¸ì§€ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        result["errors"].append("images/ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
        result["valid"] = False
    
    # sparse í´ë” í™•ì¸
    sparse_dir = source_dir / "sparse"
    if sparse_dir.exists():
        # sparse/0 ë˜ëŠ” sparse/ ì§ì ‘ í™•ì¸
        sparse_0 = sparse_dir / "0"
        if sparse_0.exists():
            result["sparse_dir"] = sparse_0
        else:
            result["sparse_dir"] = sparse_dir
        
        # COLMAP íŒŒì¼ í™•ì¸
        check_dir = result["sparse_dir"]
        required_files = ["cameras.bin", "images.bin", "points3D.bin"]
        for f in required_files:
            if not (check_dir / f).exists():
                # .txt í˜•ì‹ë„ í—ˆìš©
                if not (check_dir / f.replace(".bin", ".txt")).exists():
                    result["warnings"].append(f"sparse í´ë”ì— {f} íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
    else:
        result["warnings"].append("sparse/ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤ (COLMAP ê²°ê³¼ í•„ìš”).")
    
    # dense í´ë” í™•ì¸ (ì„ íƒì‚¬í•­)
    dense_dir = source_dir / "dense"
    if dense_dir.exists():
        result["dense_dir"] = dense_dir
    
    return result


def get_all_files(source_dir: Path) -> List[Path]:
    """ì¬ê·€ì ìœ¼ë¡œ ëª¨ë“  íŒŒì¼ ìˆ˜ì§‘"""
    files = []
    for path in source_dir.rglob("*"):
        if path.is_file():
            files.append(path)
    return sorted(files)


def check_object_exists(s3_client, bucket: str, key: str) -> bool:
    """MinIOì— ê°ì²´ê°€ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
    try:
        s3_client.head_object(Bucket=bucket, Key=key)
        return True
    except ClientError as e:
        if e.response["Error"]["Code"] == "404":
            return False
        raise


def upload_nvs_data(
    source_dir: str,
    bucket: str = "raw-data-nvs",
    project_name: Optional[str] = None,
    date_prefix: Optional[str] = None,
    skip_existing: bool = True,
    validate: bool = True,
    dry_run: bool = False
) -> Tuple[int, int, int]:
    """
    NVS ë°ì´í„°ë¥¼ MinIOì— ì—…ë¡œë“œ
    
    Args:
        source_dir: COLMAP ì²˜ë¦¬ëœ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬
        bucket: ëŒ€ìƒ MinIO ë²„í‚·
        project_name: í”„ë¡œì íŠ¸ëª… (prefix)
        date_prefix: ë‚ ì§œ prefix (ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ)
        skip_existing: ì´ë¯¸ ìˆëŠ” íŒŒì¼ ìŠ¤í‚µ
        validate: COLMAP êµ¬ì¡° ê²€ì¦
        dry_run: ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ í…ŒìŠ¤íŠ¸
    
    Returns:
        (ì—…ë¡œë“œ ìˆ˜, ìŠ¤í‚µ ìˆ˜, ì‹¤íŒ¨ ìˆ˜)
    """
    source_path = Path(source_dir).resolve()
    
    print("=" * 60)
    print("ğŸ“¤ NVS ë°ì´í„° ì—…ë¡œë“œ (COLMAP êµ¬ì¡°)")
    print("=" * 60)
    
    # ì†ŒìŠ¤ ê²€ì¦
    if not source_path.exists():
        raise FileNotFoundError(f"ì†ŒìŠ¤ ë””ë ‰í† ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤: {source_path}")
    
    # COLMAP êµ¬ì¡° ê²€ì¦
    if validate:
        print("\nğŸ” COLMAP í´ë” êµ¬ì¡° ê²€ì¦...")
        validation = validate_colmap_structure(source_path)
        
        if validation["images_dir"]:
            print(f"   âœ… images/: {validation.get('image_count', 0)}ê°œ ì´ë¯¸ì§€")
        if validation["sparse_dir"]:
            print(f"   âœ… sparse/: {validation['sparse_dir'].relative_to(source_path)}")
        if validation["dense_dir"]:
            print(f"   âœ… dense/: ìˆìŒ")
        
        for warn in validation["warnings"]:
            print(f"   âš ï¸  {warn}")
        for err in validation["errors"]:
            print(f"   âŒ {err}")
        
        if not validation["valid"]:
            print("\nâŒ COLMAP êµ¬ì¡° ê²€ì¦ ì‹¤íŒ¨. --no-validate ì˜µì…˜ìœ¼ë¡œ ìš°íšŒ ê°€ëŠ¥.")
            return 0, 0, 0
    
    # Prefix ìƒì„±
    if date_prefix is None:
        date_prefix = datetime.now().strftime("%Y%m%d")
    
    if project_name:
        prefix = f"{project_name}/{date_prefix}"
    else:
        # ì†ŒìŠ¤ í´ë” ì´ë¦„ì„ í”„ë¡œì íŠ¸ëª…ìœ¼ë¡œ ì‚¬ìš©
        project_name = source_path.name
        prefix = f"{project_name}/{date_prefix}"
    
    # MinIO ì„¤ì •
    config = load_minio_config()
    print(f"\nğŸ“¡ MinIO Endpoint: {config['endpoint_url']}")
    print(f"ğŸª£ Target Bucket: {bucket}")
    print(f"ğŸ“ Source: {source_path}")
    print(f"ğŸ“‚ Prefix: {prefix}/")
    
    # S3 í´ë¼ì´ì–¸íŠ¸
    s3_client = create_s3_client(config)
    
    # ë²„í‚· í™•ì¸/ìƒì„±
    ensure_bucket_exists(s3_client, bucket)
    
    # íŒŒì¼ ëª©ë¡ ìˆ˜ì§‘
    print("\nğŸ” íŒŒì¼ ìŠ¤ìº” ì¤‘...")
    files = get_all_files(source_path)
    
    if not files:
        print("âš ï¸  ì—…ë¡œë“œí•  íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return 0, 0, 0
    
    # í†µê³„
    total_size = sum(f.stat().st_size for f in files)
    print(f"   ğŸ“Š ì´ {len(files)}ê°œ íŒŒì¼, {total_size / (1024*1024):.2f} MB")
    
    # íŒŒì¼ íƒ€ì…ë³„ í†µê³„
    file_types = {}
    for f in files:
        ext = f.suffix.lower() or "(no ext)"
        file_types[ext] = file_types.get(ext, 0) + 1
    
    print(f"   ğŸ“„ íŒŒì¼ íƒ€ì…: {', '.join(f'{k}({v})' for k, v in sorted(file_types.items()))}")
    
    if dry_run:
        print("\nğŸ§ª Dry Run ëª¨ë“œ - ì‹¤ì œ ì—…ë¡œë“œ ì—†ìŒ")
        print("\nì—…ë¡œë“œ ì˜ˆì • íŒŒì¼ (ì²˜ìŒ 10ê°œ):")
        for f in files[:10]:
            rel_path = f.relative_to(source_path)
            key = f"{prefix}/{rel_path}"
            print(f"   ğŸ“„ s3://{bucket}/{key}")
        if len(files) > 10:
            print(f"   ... ì™¸ {len(files) - 10}ê°œ íŒŒì¼")
        return len(files), 0, 0
    
    # ì—…ë¡œë“œ ì‹¤í–‰
    print("\nğŸ“¤ ì—…ë¡œë“œ ì‹œì‘...")
    
    uploaded = 0
    skipped = 0
    failed = 0
    
    with tqdm(total=total_size, unit="B", unit_scale=True, desc="ì „ì²´ ì§„í–‰ë¥ ") as pbar:
        for local_file in files:
            rel_path = local_file.relative_to(source_path)
            key = f"{prefix}/{rel_path}"
            file_size = local_file.stat().st_size
            
            # ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ìŠ¤í‚µ
            if skip_existing and check_object_exists(s3_client, bucket, key):
                skipped += 1
                pbar.update(file_size)
                continue
            
            # ì—…ë¡œë“œ
            try:
                def progress_callback(bytes_transferred):
                    pbar.update(bytes_transferred)
                
                s3_client.upload_file(
                    str(local_file),
                    bucket,
                    key,
                    Callback=progress_callback
                )
                uploaded += 1
            except Exception as e:
                print(f"\nâŒ ì—…ë¡œë“œ ì‹¤íŒ¨: {rel_path} - {e}")
                failed += 1
                pbar.update(file_size)
    
    # ê²°ê³¼ ì¶œë ¥
    print("\n" + "=" * 60)
    print("âœ… ì—…ë¡œë“œ ì™„ë£Œ!")
    print("=" * 60)
    print(f"   ğŸ“¤ ì—…ë¡œë“œë¨: {uploaded}ê°œ")
    print(f"   â­ï¸  ìŠ¤í‚µë¨: {skipped}ê°œ")
    print(f"   âŒ ì‹¤íŒ¨: {failed}ê°œ")
    print(f"\nğŸ“‚ ì—…ë¡œë“œ ê²½ë¡œ: s3://{bucket}/{prefix}/")
    print(f"ğŸ“Œ MinIO Console: http://localhost:9001/browser/{bucket}/{prefix}/")
    
    # ë©”íƒ€ë°ì´í„° íŒŒì¼ ìƒì„± ë° ì—…ë¡œë“œ
    meta_content = f"""# NVS Dataset Metadata
project: {project_name}
upload_date: {datetime.now().isoformat()}
source_path: {source_path}
total_files: {len(files)}
total_size_mb: {total_size / (1024*1024):.2f}

## Structure
- images: {validation.get('image_count', 'N/A')} files
- sparse: {'yes' if validation.get('sparse_dir') else 'no'}
- dense: {'yes' if validation.get('dense_dir') else 'no'}
"""
    
    meta_key = f"{prefix}/_metadata.md"
    try:
        s3_client.put_object(
            Bucket=bucket,
            Key=meta_key,
            Body=meta_content.encode('utf-8'),
            ContentType='text/markdown'
        )
        print(f"   ğŸ“ ë©”íƒ€ë°ì´í„°: s3://{bucket}/{meta_key}")
    except Exception as e:
        print(f"   âš ï¸ ë©”íƒ€ë°ì´í„° ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return uploaded, skipped, failed


def main():
    parser = argparse.ArgumentParser(
        description="NVS ë°ì´í„°(COLMAP êµ¬ì¡°) MinIO ì—…ë¡œë“œ",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # ê¸°ë³¸ ì—…ë¡œë“œ (í”„ë¡œì íŠ¸ëª…: í´ë”ì´ë¦„, ë‚ ì§œ: ì˜¤ëŠ˜)
  python scripts/upload_nvs_data.py --source ./my_scene

  # í”„ë¡œì íŠ¸ëª…ê³¼ ë‚ ì§œ ì§€ì •
  python scripts/upload_nvs_data.py -s ./scene -p campus_building --date 2024-01-15

  # ê²€ì¦ ì—†ì´ ì—…ë¡œë“œ
  python scripts/upload_nvs_data.py -s ./data --no-validate

  # Dry run (í…ŒìŠ¤íŠ¸)
  python scripts/upload_nvs_data.py -s ./data -p test --dry-run
        """
    )
    
    parser.add_argument(
        "--source", "-s",
        type=str,
        required=True,
        help="COLMAP ì²˜ë¦¬ëœ ì†ŒìŠ¤ ë””ë ‰í† ë¦¬"
    )
    
    parser.add_argument(
        "--bucket", "-b",
        type=str,
        default="raw-data-nvs",
        help="ëŒ€ìƒ MinIO ë²„í‚· (ê¸°ë³¸ê°’: raw-data-nvs)"
    )
    
    parser.add_argument(
        "--project", "-p",
        type=str,
        default=None,
        help="í”„ë¡œì íŠ¸ëª… (prefix). ì—†ìœ¼ë©´ ì†ŒìŠ¤ í´ë” ì´ë¦„ ì‚¬ìš©"
    )
    
    parser.add_argument(
        "--date", "-d",
        type=str,
        default=None,
        help="ë‚ ì§œ prefix (YYYYMMDD í˜•ì‹). ì—†ìœ¼ë©´ ì˜¤ëŠ˜ ë‚ ì§œ"
    )
    
    parser.add_argument(
        "--skip-existing",
        action="store_true",
        default=True,
        help="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ ìŠ¤í‚µ (ê¸°ë³¸ê°’: True)"
    )
    
    parser.add_argument(
        "--no-skip",
        action="store_true",
        help="ì´ë¯¸ ì¡´ì¬í•˜ëŠ” íŒŒì¼ë„ ë®ì–´ì“°ê¸°"
    )
    
    parser.add_argument(
        "--no-validate",
        action="store_true",
        help="COLMAP êµ¬ì¡° ê²€ì¦ ìŠ¤í‚µ"
    )
    
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="ì‹¤ì œ ì—…ë¡œë“œ ì—†ì´ í…ŒìŠ¤íŠ¸"
    )
    
    args = parser.parse_args()
    
    # ì—…ë¡œë“œ ì‹¤í–‰
    upload_nvs_data(
        source_dir=args.source,
        bucket=args.bucket,
        project_name=args.project,
        date_prefix=args.date,
        skip_existing=not args.no_skip,
        validate=not args.no_validate,
        dry_run=args.dry_run
    )


if __name__ == "__main__":
    main()
