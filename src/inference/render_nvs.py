#!/usr/bin/env python3
"""
NVS (3D Gaussian Splatting) ë Œë”ë§ ìŠ¤í¬ë¦½íŠ¸
í•™ìŠµëœ ëª¨ë¸ì„ ë¶ˆëŸ¬ì™€ ì¹´ë©”ë¼ ê²½ë¡œë¥¼ ë”°ë¼ ì˜ìƒ ìƒì„±

ì‚¬ìš©ë²•:
    # MLflow Run IDë¡œ ëª¨ë¸ ë¡œë“œ
    python src/inference/render_nvs.py --run-id abc123 --camera-path cameras.json -o output/
    
    # ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë“œ
    python src/inference/render_nvs.py --checkpoint output/point_cloud.ply --camera-path cameras.json
    
    # 360ë„ ìë™ ìƒì„±
    python src/inference/render_nvs.py --run-id abc123 --auto-orbit --num-frames 120
"""

import argparse
import json
import os
import shutil
import subprocess
import sys
import tempfile
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Optional, Tuple

import cv2
import numpy as np
import torch
from PIL import Image
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

# .env ë¡œë“œ
from dotenv import load_dotenv
env_file = PROJECT_ROOT / ".env"
if env_file.exists():
    load_dotenv(env_file)

from src.models.gaussian_model import GaussianModel, GaussianModelConfig


# ============================================
# ì¹´ë©”ë¼ ê²½ë¡œ ì²˜ë¦¬
# ============================================
def load_camera_path(json_path: str) -> List[Dict]:
    """
    ì¹´ë©”ë¼ ê²½ë¡œ JSON ë¡œë“œ
    
    JSON í˜•ì‹:
    {
        "camera_path": [
            {
                "camera_to_world": [[...], [...], [...], [...]],  # 4x4 í–‰ë ¬
                "fov": 60,  # FOV in degrees
                "aspect": 1.777  # width/height
            },
            ...
        ],
        "render_height": 720,
        "render_width": 1280
    }
    """
    with open(json_path, 'r') as f:
        data = json.load(f)
    
    if "camera_path" in data:
        return data["camera_path"], data.get("render_width", 1280), data.get("render_height", 720)
    else:
        # ë‹¨ìˆœ ë¦¬ìŠ¤íŠ¸ í˜•ì‹
        return data, 1280, 720


def generate_orbit_cameras(
    num_frames: int = 120,
    radius: float = 3.0,
    height: float = 0.5,
    target: Tuple[float, float, float] = (0, 0, 0),
    fov: float = 60.0
) -> List[Dict]:
    """
    360ë„ orbit ì¹´ë©”ë¼ ê²½ë¡œ ìƒì„±
    """
    cameras = []
    
    for i in range(num_frames):
        angle = 2 * np.pi * i / num_frames
        
        # ì¹´ë©”ë¼ ìœ„ì¹˜
        cam_pos = np.array([
            radius * np.cos(angle) + target[0],
            height + target[1],
            radius * np.sin(angle) + target[2]
        ])
        
        # Look-at ë³€í™˜
        target_vec = np.array(target)
        up = np.array([0, 1, 0])
        
        forward = target_vec - cam_pos
        forward = forward / np.linalg.norm(forward)
        right = np.cross(forward, up)
        right = right / (np.linalg.norm(right) + 1e-8)
        up = np.cross(right, forward)
        
        # Camera-to-World ë³€í™˜ (4x4)
        c2w = np.eye(4)
        c2w[:3, 0] = right
        c2w[:3, 1] = up
        c2w[:3, 2] = -forward
        c2w[:3, 3] = cam_pos
        
        cameras.append({
            "camera_to_world": c2w.tolist(),
            "fov": fov,
            "aspect": 16/9
        })
    
    return cameras


def camera_to_view_params(camera_data: Dict, width: int, height: int, device: str = "cuda"):
    """
    ì¹´ë©”ë¼ ë°ì´í„°ë¥¼ ë Œë”ë§ìš© íŒŒë¼ë¯¸í„°ë¡œ ë³€í™˜
    """
    c2w = np.array(camera_data["camera_to_world"])
    fov = camera_data.get("fov", 60)
    
    # World-to-Camera (c2wì˜ ì—­í–‰ë ¬)
    w2c = np.linalg.inv(c2w)
    R = w2c[:3, :3]
    T = w2c[:3, 3]
    
    # FOVë¥¼ ë¼ë””ì•ˆìœ¼ë¡œ
    fov_rad = np.deg2rad(fov)
    FoVx = fov_rad
    FoVy = fov_rad * height / width
    
    return {
        "R": R,
        "T": T,
        "FoVx": FoVx,
        "FoVy": FoVy,
        "width": width,
        "height": height
    }


# ============================================
# ë Œë”ëŸ¬
# ============================================
class NVSRenderer:
    """NVS ë Œë”ëŸ¬"""
    
    def __init__(self, gaussians: GaussianModel, device: str = "cuda"):
        self.gaussians = gaussians
        self.device = device
        self.background = torch.tensor([0, 0, 0], dtype=torch.float32, device=device)
        
        # gsplat ì²´í¬
        try:
            import gsplat
            from gsplat import rasterization
            self.gsplat_available = True
            self.rasterization = rasterization
        except ImportError:
            self.gsplat_available = False
            print("âš ï¸ gsplat ë¯¸ì„¤ì¹˜ - Mock ë Œë”ëŸ¬ ì‚¬ìš©")
    
    def render(self, camera_params: Dict) -> np.ndarray:
        """
        ë‹¨ì¼ ë·° ë Œë”ë§
        
        Returns:
            [H, W, 3] uint8 ì´ë¯¸ì§€
        """
        with torch.no_grad():
            if self.gsplat_available:
                rendered = self._render_gsplat(camera_params)
            else:
                rendered = self._render_mock(camera_params)
        
        # Tensor -> numpy uint8
        img = rendered.cpu().numpy()
        img = np.clip(img * 255, 0, 255).astype(np.uint8)
        
        return img
    
    def _render_gsplat(self, camera_params: Dict) -> torch.Tensor:
        """gsplat ë Œë”ë§"""
        H, W = camera_params["height"], camera_params["width"]
        
        # View matrix
        R = torch.tensor(camera_params["R"], dtype=torch.float32, device=self.device)
        T = torch.tensor(camera_params["T"], dtype=torch.float32, device=self.device)
        
        view_mat = torch.eye(4, device=self.device)
        view_mat[:3, :3] = R
        view_mat[:3, 3] = T
        
        # K matrix
        fx = W / (2 * np.tan(camera_params["FoVx"] / 2))
        fy = H / (2 * np.tan(camera_params["FoVy"] / 2))
        K = torch.tensor([
            [fx, 0, W/2],
            [0, fy, H/2],
            [0, 0, 1]
        ], device=self.device)
        
        # ë Œë”ë§
        try:
            renders, alphas, _ = self.rasterization(
                means=self.gaussians.xyz,
                quats=self.gaussians.rotation,
                scales=self.gaussians.scaling,
                opacities=self.gaussians.opacity.squeeze(-1),
                colors=self.gaussians.features[:, 0, :] * 0.28209479177387814 + 0.5,
                viewmats=view_mat.unsqueeze(0),
                Ks=K.unsqueeze(0),
                width=W,
                height=H,
                sh_degree=0,
                backgrounds=self.background.unsqueeze(0),
            )
            rendered = renders[0]  # [H, W, 3]
        except Exception as e:
            print(f"âš ï¸ gsplat ë Œë”ë§ ì‹¤íŒ¨: {e}")
            return self._render_mock(camera_params)
        
        return rendered
    
    def _render_mock(self, camera_params: Dict) -> torch.Tensor:
        """Mock ë Œë”ë§"""
        H, W = camera_params["height"], camera_params["width"]
        
        # Gaussian ìƒ‰ìƒì˜ ê°€ì¤‘ í‰ê· 
        colors = self.gaussians.features[:, 0, :]
        opacities = self.gaussians.opacity.squeeze(-1)
        
        rgb_colors = (colors * 0.28209479177387814 + 0.5).clamp(0, 1)
        weights = opacities.unsqueeze(-1)
        weighted_color = (rgb_colors * weights).sum(dim=0) / (weights.sum() + 1e-8)
        
        # ì „ì²´ ì´ë¯¸ì§€ë¥¼ í‰ê· ìƒ‰ìœ¼ë¡œ
        rendered = weighted_color.view(1, 1, 3).expand(H, W, 3).contiguous()
        
        return rendered


# ============================================
# MinIO ì—…ë¡œë“œ
# ============================================
class MinIOUploader:
    """MinIO ì—…ë¡œë“œ"""
    
    def __init__(self, endpoint: str = None, bucket: str = "mlflow-artifacts"):
        import boto3
        
        self.endpoint = endpoint or os.getenv("MLFLOW_S3_ENDPOINT_URL", "http://localhost:9000")
        self.bucket = bucket
        
        self.s3_client = boto3.client(
            "s3",
            endpoint_url=self.endpoint,
            aws_access_key_id=os.getenv("AWS_ACCESS_KEY_ID", "minioadmin"),
            aws_secret_access_key=os.getenv("AWS_SECRET_ACCESS_KEY", "minio_secure_password_2024"),
        )
    
    def upload(self, local_path: Path, s3_key: str) -> str:
        """íŒŒì¼ ì—…ë¡œë“œ ë° ë‹¤ìš´ë¡œë“œ ë§í¬ ë°˜í™˜"""
        self.s3_client.upload_file(str(local_path), self.bucket, s3_key)
        
        # ë‹¤ìš´ë¡œë“œ ë§í¬ ìƒì„± (presigned URL)
        url = self.s3_client.generate_presigned_url(
            'get_object',
            Params={'Bucket': self.bucket, 'Key': s3_key},
            ExpiresIn=86400 * 7  # 7ì¼
        )
        
        return url
    
    def get_public_url(self, s3_key: str) -> str:
        """ê³µê°œ URL (MinIO Console)"""
        return f"{self.endpoint}/{self.bucket}/{s3_key}"


# ============================================
# ë¹„ë””ì˜¤ ìƒì„±
# ============================================
def create_video_from_frames(
    frames_dir: Path,
    output_path: Path,
    fps: int = 30,
    codec: str = "libx264",
    crf: int = 23
) -> Path:
    """
    í”„ë ˆì„ ì´ë¯¸ì§€ë“¤ë¡œ MP4 ë¹„ë””ì˜¤ ìƒì„±
    OpenCV VideoWriter ì‚¬ìš© (ffmpeg fallback)
    """
    output_path = Path(output_path)
    output_path.parent.mkdir(parents=True, exist_ok=True)
    
    # í”„ë ˆì„ íŒŒì¼ ëª©ë¡
    frame_files = sorted(frames_dir.glob("frame_*.png"))
    if not frame_files:
        raise FileNotFoundError(f"í”„ë ˆì„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {frames_dir}")
    
    print(f"ğŸ¬ ë¹„ë””ì˜¤ ìƒì„± ì¤‘: {output_path}")
    print(f"   ğŸ“Š í”„ë ˆì„ ìˆ˜: {len(frame_files)}")
    
    # ì²« í”„ë ˆì„ì—ì„œ í¬ê¸° í™•ì¸
    first_frame = cv2.imread(str(frame_files[0]))
    height, width = first_frame.shape[:2]
    
    # OpenCV VideoWriterë¡œ ë¹„ë””ì˜¤ ìƒì„±
    # mp4v ì½”ë± ì‚¬ìš© (ëŒ€ë¶€ë¶„ì˜ í™˜ê²½ì—ì„œ í˜¸í™˜)
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    
    # .mp4 í™•ì¥ì ë³´ì¥
    if output_path.suffix.lower() != '.mp4':
        output_path = output_path.with_suffix('.mp4')
    
    writer = cv2.VideoWriter(str(output_path), fourcc, fps, (width, height))
    
    if not writer.isOpened():
        print("   âš ï¸ OpenCV VideoWriter ì‹¤íŒ¨, ffmpeg ì‹œë„...")
        return _create_video_ffmpeg(frames_dir, output_path, fps, codec, crf)
    
    # í”„ë ˆì„ ì“°ê¸°
    for frame_path in tqdm(frame_files, desc="ë¹„ë””ì˜¤ ìƒì„±"):
        frame = cv2.imread(str(frame_path))
        writer.write(frame)
    
    writer.release()
    print(f"   âœ… ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ (OpenCV)")
    
    return output_path


def _create_video_ffmpeg(
    frames_dir: Path,
    output_path: Path,
    fps: int = 30,
    codec: str = "libx264",
    crf: int = 23
) -> Path:
    """ffmpegë¡œ ë¹„ë””ì˜¤ ìƒì„± (fallback)"""
    cmd = [
        "ffmpeg", "-y",
        "-framerate", str(fps),
        "-i", str(frames_dir / "frame_%06d.png"),
        "-c:v", codec,
        "-pix_fmt", "yuv420p",
        "-crf", str(crf),
        str(output_path)
    ]
    
    try:
        result = subprocess.run(
            cmd,
            capture_output=True,
            text=True,
            check=True
        )
        print(f"   âœ… ë¹„ë””ì˜¤ ìƒì„± ì™„ë£Œ (ffmpeg)")
    except (subprocess.CalledProcessError, FileNotFoundError) as e:
        print(f"   âŒ ffmpeg ì‹¤íŒ¨: {e}")
        raise RuntimeError("ë¹„ë””ì˜¤ ìƒì„± ì‹¤íŒ¨. OpenCVì™€ ffmpeg ëª¨ë‘ ì‚¬ìš© ë¶ˆê°€.")
    
    return output_path


# ============================================
# ëª¨ë¸ ë¡œë“œ
# ============================================
def load_model_from_mlflow(run_id: str, device: str = "cuda") -> GaussianModel:
    """MLflow Runì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    import mlflow
    
    tracking_uri = os.getenv("MLFLOW_TRACKING_URI", "http://localhost:5000")
    mlflow.set_tracking_uri(tracking_uri)
    
    print(f"ğŸ“¡ MLflow: {tracking_uri}")
    print(f"ğŸ” Run ID: {run_id}")
    
    client = mlflow.tracking.MlflowClient()
    
    # ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ
    artifact_path = client.download_artifacts(run_id, "model")
    print(f"   ğŸ“¥ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ: {artifact_path}")
    
    # PLY íŒŒì¼ ì°¾ê¸°
    artifact_dir = Path(artifact_path)
    ply_files = list(artifact_dir.glob("*.ply"))
    
    if not ply_files:
        raise FileNotFoundError(f"Run {run_id}ì—ì„œ PLY íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    ply_path = ply_files[0]
    return load_model_from_ply(str(ply_path), device)


def load_model_from_ply(ply_path: str, device: str = "cuda") -> GaussianModel:
    """PLY íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ"""
    print(f"ğŸ“¦ PLY ë¡œë“œ: {ply_path}")
    
    # PLY íŒŒì‹± (ASCII í˜•ì‹)
    with open(ply_path, 'r') as f:
        lines = f.readlines()
    
    # í—¤ë” íŒŒì‹±
    header_end = 0
    num_vertices = 0
    for i, line in enumerate(lines):
        if line.strip() == "end_header":
            header_end = i + 1
            break
        if line.startswith("element vertex"):
            num_vertices = int(line.split()[-1])
    
    print(f"   ğŸ“Š Vertices: {num_vertices}")
    
    # ë°ì´í„° íŒŒì‹±
    points = []
    colors = []
    opacities = []
    scales = []
    rotations = []
    
    for line in lines[header_end:]:
        parts = line.strip().split()
        if len(parts) < 14:
            continue
        
        x, y, z = float(parts[0]), float(parts[1]), float(parts[2])
        r, g, b = int(parts[3]), int(parts[4]), int(parts[5])
        opacity = float(parts[6])
        s0, s1, s2 = float(parts[7]), float(parts[8]), float(parts[9])
        r0, r1, r2, r3 = float(parts[10]), float(parts[11]), float(parts[12]), float(parts[13])
        
        points.append([x, y, z])
        colors.append([r/255.0, g/255.0, b/255.0])
        opacities.append(opacity)
        scales.append([s0, s1, s2])
        rotations.append([r0, r1, r2, r3])
    
    # Gaussian ëª¨ë¸ ìƒì„±
    config = GaussianModelConfig()
    model = GaussianModel(config)
    
    model.num_gaussians = len(points)
    
    import torch.nn as nn
    
    model._xyz = nn.Parameter(
        torch.tensor(points, dtype=torch.float32, device=device)
    )
    
    # Colors to SH DC
    colors_tensor = torch.tensor(colors, dtype=torch.float32, device=device)
    C0 = 0.28209479177387814
    sh_dc = (colors_tensor - 0.5) / C0
    model._features_dc = nn.Parameter(sh_dc.unsqueeze(1))
    
    # ë‚˜ë¨¸ì§€ SH ê³„ìˆ˜ (0)
    num_sh_rest = (config.sh_degree + 1) ** 2 - 1
    model._features_rest = nn.Parameter(
        torch.zeros(model.num_gaussians, num_sh_rest, 3, device=device)
    )
    
    # Opacity (logit ì—­ë³€í™˜)
    opacities_tensor = torch.tensor(opacities, dtype=torch.float32, device=device)
    model._opacity = nn.Parameter(
        torch.logit(opacities_tensor.clamp(1e-5, 1-1e-5)).unsqueeze(-1)
    )
    
    # Scales (ì´ë¯¸ log space)
    model._scaling = nn.Parameter(
        torch.tensor(scales, dtype=torch.float32, device=device)
    )
    
    # Rotations
    model._rotation = nn.Parameter(
        torch.tensor(rotations, dtype=torch.float32, device=device)
    )
    
    print(f"   âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model.num_gaussians} gaussians")
    
    return model


# ============================================
# ë©”ì¸ ë Œë”ë§ í•¨ìˆ˜
# ============================================
def render_video(
    model: GaussianModel,
    cameras: List[Dict],
    output_dir: Path,
    width: int = 1280,
    height: int = 720,
    fps: int = 30,
    device: str = "cuda",
    upload_to_minio: bool = True
) -> Dict:
    """
    ì¹´ë©”ë¼ ê²½ë¡œë¥¼ ë”°ë¼ ë¹„ë””ì˜¤ ë Œë”ë§
    """
    output_dir = Path(output_dir)
    frames_dir = output_dir / "frames"
    frames_dir.mkdir(parents=True, exist_ok=True)
    
    renderer = NVSRenderer(model, device)
    
    print(f"\nğŸ¬ ë Œë”ë§ ì‹œì‘: {len(cameras)} í”„ë ˆì„")
    print(f"   ğŸ“ í•´ìƒë„: {width}x{height}")
    
    # í”„ë ˆì„ ë Œë”ë§
    for i, cam_data in enumerate(tqdm(cameras, desc="ë Œë”ë§")):
        camera_params = camera_to_view_params(cam_data, width, height, device)
        
        frame = renderer.render(camera_params)
        
        # BGRë¡œ ë³€í™˜í•˜ì—¬ ì €ì¥
        frame_path = frames_dir / f"frame_{i:06d}.png"
        cv2.imwrite(str(frame_path), cv2.cvtColor(frame, cv2.COLOR_RGB2BGR))
    
    # ë¹„ë””ì˜¤ ìƒì„±
    video_name = f"render_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp4"
    video_path = output_dir / video_name
    
    create_video_from_frames(frames_dir, video_path, fps=fps)
    
    result = {
        "video_path": str(video_path),
        "num_frames": len(cameras),
        "resolution": f"{width}x{height}",
        "fps": fps
    }
    
    # MinIO ì—…ë¡œë“œ
    if upload_to_minio:
        try:
            uploader = MinIOUploader()
            s3_key = f"nvs-renders/{video_name}"
            download_url = uploader.upload(video_path, s3_key)
            
            result["s3_key"] = s3_key
            result["download_url"] = download_url
            
            print(f"\nğŸ“¤ MinIO ì—…ë¡œë“œ ì™„ë£Œ")
            print(f"   ğŸ”— ë‹¤ìš´ë¡œë“œ ë§í¬ (7ì¼ ìœ íš¨):")
            print(f"   {download_url}")
        except Exception as e:
            print(f"\nâš ï¸ MinIO ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
    
    return result


# ============================================
# CLI
# ============================================
def main():
    parser = argparse.ArgumentParser(
        description="NVS ë Œë”ë§ (3D Gaussian Splatting)",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ì˜ˆì‹œ:
  # MLflow Runì—ì„œ ëª¨ë¸ ë¡œë“œ + ì¹´ë©”ë¼ ê²½ë¡œ
  python src/inference/render_nvs.py --run-id abc123 --camera-path cameras.json
  
  # ì²´í¬í¬ì¸íŠ¸ ì§ì ‘ ë¡œë“œ + 360ë„ ìë™ ìƒì„±
  python src/inference/render_nvs.py --checkpoint point_cloud.ply --auto-orbit --num-frames 120
  
  # ê³ í•´ìƒë„ ë Œë”ë§
  python src/inference/render_nvs.py --run-id abc123 --auto-orbit --width 1920 --height 1080
        """
    )
    
    # ëª¨ë¸ ì†ŒìŠ¤
    model_group = parser.add_mutually_exclusive_group(required=True)
    model_group.add_argument(
        "--run-id", "-r",
        type=str,
        help="MLflow Run ID"
    )
    model_group.add_argument(
        "--checkpoint", "-c",
        type=str,
        help="PLY ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ"
    )
    
    # ì¹´ë©”ë¼ ê²½ë¡œ
    camera_group = parser.add_mutually_exclusive_group(required=True)
    camera_group.add_argument(
        "--camera-path", "-p",
        type=str,
        help="ì¹´ë©”ë¼ ê²½ë¡œ JSON íŒŒì¼"
    )
    camera_group.add_argument(
        "--auto-orbit",
        action="store_true",
        help="360ë„ orbit ì¹´ë©”ë¼ ìë™ ìƒì„±"
    )
    
    # ì¶œë ¥
    parser.add_argument(
        "--output", "-o",
        type=str,
        default="./output/renders",
        help="ì¶œë ¥ ë””ë ‰í† ë¦¬"
    )
    
    # ë Œë”ë§ ì˜µì…˜
    parser.add_argument("--width", type=int, default=1280, help="ë Œë”ë§ ë„ˆë¹„")
    parser.add_argument("--height", type=int, default=720, help="ë Œë”ë§ ë†’ì´")
    parser.add_argument("--fps", type=int, default=30, help="ë¹„ë””ì˜¤ FPS")
    parser.add_argument("--num-frames", type=int, default=120, help="Orbit í”„ë ˆì„ ìˆ˜")
    
    # Orbit ì˜µì…˜
    parser.add_argument("--radius", type=float, default=3.0, help="Orbit ë°˜ê²½")
    parser.add_argument("--height-offset", type=float, default=0.5, help="ì¹´ë©”ë¼ ë†’ì´ ì˜¤í”„ì…‹")
    parser.add_argument("--fov", type=float, default=60.0, help="FOV (degrees)")
    
    # ê¸°íƒ€
    parser.add_argument("--no-upload", action="store_true", help="MinIO ì—…ë¡œë“œ ì•ˆ í•¨")
    parser.add_argument("--device", type=str, default="auto", help="ë””ë°”ì´ìŠ¤")
    
    args = parser.parse_args()
    
    print("=" * 60)
    print("ğŸ¬ NVS ë Œë”ë§ (3D Gaussian Splatting)")
    print("=" * 60)
    
    # ë””ë°”ì´ìŠ¤
    if args.device == "auto":
        device = "cuda" if torch.cuda.is_available() else "cpu"
    else:
        device = args.device
    print(f"ğŸ“± Device: {device}")
    
    # ëª¨ë¸ ë¡œë“œ
    if args.run_id:
        model = load_model_from_mlflow(args.run_id, device)
    else:
        model = load_model_from_ply(args.checkpoint, device)
    
    # ì¹´ë©”ë¼ ê²½ë¡œ
    if args.camera_path:
        cameras, width, height = load_camera_path(args.camera_path)
        # CLI ì˜µì…˜ìœ¼ë¡œ ì˜¤ë²„ë¼ì´ë“œ ê°€ëŠ¥
        if args.width != 1280:
            width = args.width
        if args.height != 720:
            height = args.height
    else:
        # ìë™ orbit ìƒì„±
        cameras = generate_orbit_cameras(
            num_frames=args.num_frames,
            radius=args.radius,
            height=args.height_offset,
            fov=args.fov
        )
        width, height = args.width, args.height
    
    print(f"   ğŸ“· ì¹´ë©”ë¼: {len(cameras)} ë·°")
    
    # ë Œë”ë§
    result = render_video(
        model=model,
        cameras=cameras,
        output_dir=Path(args.output),
        width=width,
        height=height,
        fps=args.fps,
        device=device,
        upload_to_minio=not args.no_upload
    )
    
    print("\n" + "=" * 60)
    print("âœ… ë Œë”ë§ ì™„ë£Œ!")
    print("=" * 60)
    print(f"   ğŸ¥ ë¹„ë””ì˜¤: {result['video_path']}")
    print(f"   ğŸ“Š í”„ë ˆì„: {result['num_frames']}")
    print(f"   ğŸ“ í•´ìƒë„: {result['resolution']}")
    
    if "download_url" in result:
        print(f"\nğŸ”— ë‹¤ìš´ë¡œë“œ ë§í¬:")
        print(f"   {result['download_url']}")


if __name__ == "__main__":
    main()
